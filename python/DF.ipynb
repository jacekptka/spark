{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie DataFrame'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# json\n",
    "people = spark.read.json(data_path+'people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csv\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|    _c0| _c1|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolekcja Row'ów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson1 = Row(name='Greg', age=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=32, name='Greg')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greg'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'age' in newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson = Row(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson2 = newPerson(24, 'Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=24, name='Alice')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson3 = newPerson(None, None)\n",
    "newPerson4 = newPerson(33, None)\n",
    "newPerson5 = newPerson(None, 'Peter')\n",
    "newPerson6 = newPerson(32, 'Peter')\n",
    "newPerson7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeopleDF = spark.createDataFrame([newPerson1, newPerson2, newPerson3, newPerson4, \n",
    "                                     newPerson5, newPerson6, newPerson7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  32| Greg|\n",
      "|  24|Alice|\n",
      "|null| null|\n",
      "|  33| null|\n",
      "|null|Peter|\n",
      "|  32|Peter|\n",
      "|  40| Greg|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne lokalne kolekcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typy danych: http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "Kilka podstawowych: IntegerType, DoubleType, FloatType, StringType, BooleanType, NullType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definicja schematu\n",
    "# StructType ~ Row\n",
    "schema = StructType([StructField(\"V1\", IntegerType()), StructField(\"V2\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lokalna kolekcja - lista list\n",
    "df = spark.createDataFrame([[1,2],[3,4]], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| V1| V2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przechodzenie RDD <-> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None),\n",
       " Row(age=31, name=None)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF -> RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None),\n",
       " Row(age=31, name=None)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 'Michael'),\n",
       " (30, 'Andy'),\n",
       " (19, 'Justin'),\n",
       " (35, 'Emma'),\n",
       " (None, None),\n",
       " (31, None)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.rdd.map(tuple).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD -> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|  _1|     _2|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.rdd.map(tuple).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  3|  4|\n",
      "|  4|  5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  3|  4|\n",
      "|  4|  5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do toDF można podać schemat\n",
    "schema = StructType([StructField(\"A\", IntegerType()), StructField(\"B\", StringType())])\n",
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(A=0, B='1')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na dwa sposoby stwórz DataFrame z 3 wierszami i 3 kolumnami - dwie typu string, jedna numeryczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| S1| S2| I1|\n",
      "+---+---+---+\n",
      "|  0|  1|  2|\n",
      "|  1|  2|  3|\n",
      "|  2|  3|  4|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema1 = StructType([StructField(\"S1\", StringType()), StructField(\"S2\", StringType()), StructField(\"I1\", IntegerType())])\n",
    "sc.parallelize([(x, x+1, x+2) for x in range(3)]).toDF(schema1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| s1| s2| i1|\n",
      "+---+---+---+\n",
      "|  a|  b|  2|\n",
      "|  c|  D|  3|\n",
      "|  E|  f|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constr = Row(\"s1\",\"s2\",\"i1\")\n",
    "r1 = constr(\"a\",\"b\",2)\n",
    "r2 = constr(\"c\",\"D\",3)\n",
    "r3 = constr(\"E\",\"f\",5)\n",
    "spark.createDataFrame([r1,r2,r3]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Z podanego RDD utwórz DataFrame z nazwanymi kolumnami `name` i `age` oraz odpowiednimi typami (string i int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.textFile(data_path+\"people.txt\")\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.map(lambda x: x.split(\", \")).collect()\n",
    "schema2 = StructType([StructField(\"Name\", StringType()), StructField(\"Age\", IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## wszystko w stringach\n",
    "#myRDD.map(lambda x: x.split(\", \")).toDF().printSchema()\n",
    "\n",
    "myRDD.map(lambda x: x.split(\", \")).map(lambda x: (x[0], int(x[1]))).toDF(schema2).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praca z DataFrame'ami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogolne wiadomosci na temat danych\n",
    "printSchema, show, columns, dtypes <br>\n",
    "Znane z RDD np: count, take, head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|    _c0| _c1|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "+-------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba wierszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kolumn wraz z typami danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPeopleDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odwolania do poszczegolnych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dodanie/usuniecie kolumny\n",
    "withColumn, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+\n",
      "| age|   name|ageNextYear|\n",
      "+----+-------+-----------+\n",
      "|null|Michael|       null|\n",
      "|  30|   Andy|         31|\n",
      "|  19| Justin|         20|\n",
      "|  35|   Emma|         36|\n",
      "|null|   null|       null|\n",
      "|  31|   null|         32|\n",
      "+----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(colName = 'ageNextYear', col = people.age +1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|   Emma|\n",
      "|   null|\n",
      "|   null|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiany nazwy kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podstawowe statystyki kolumn w DataFrame'ie.\n",
    "describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------+\n",
      "|summary|              age|   name|\n",
      "+-------+-----------------+-------+\n",
      "|  count|                4|      4|\n",
      "|   mean|            28.75|   null|\n",
      "| stddev|6.849574196011505|   null|\n",
      "|    min|               19|   Andy|\n",
      "|    max|               35|Michael|\n",
      "+-------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|            28.75|\n",
      "| stddev|6.849574196011505|\n",
      "|    min|               19|\n",
      "|    max|               35|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.describe('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braki danych\n",
    "isNull, isNotNull<br>\n",
    "fillna, dropna, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  32| Greg|\n",
      "|  24|Alice|\n",
      "|null| null|\n",
      "|  33| null|\n",
      "|null|Peter|\n",
      "|  32|Peter|\n",
      "|  40| Greg|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|null| null|\n",
      "|null|Peter|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.filter(newPeopleDF.age.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 24|Alice|\n",
      "| 33| null|\n",
      "| 32|Peter|\n",
      "| 40| Greg|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.filter(newPeopleDF.age.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 24|Alice|\n",
      "| -1| null|\n",
      "| 33| null|\n",
      "| -1|Peter|\n",
      "| 32|Peter|\n",
      "| 40| Greg|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.fillna(-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 32|   Greg|\n",
      "| 24|  Alice|\n",
      "| -1|unknown|\n",
      "| 33|unknown|\n",
      "| -1|  Peter|\n",
      "| 32|  Peter|\n",
      "| 40|   Greg|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.fillna({'age':-1, 'name':'unknown'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  32| Greg|\n",
      "|  24|Alice|\n",
      "|null|   NN|\n",
      "|  33|   NN|\n",
      "|null|Peter|\n",
      "|  32|Peter|\n",
      "|  40| Greg|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.fillna({'name':'unknown'}).replace('unknown', 'NN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 24|Alice|\n",
      "| 32|Peter|\n",
      "| 40| Greg|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 24|Alice|\n",
      "| 33| null|\n",
      "| 32|Peter|\n",
      "| 40| Greg|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.dropna(subset='age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje wprost ze skladni SQL \n",
    "select, where (alias filter), orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Justin| 19|\n",
      "|  Andy| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.\\\n",
    "select('name', 'age').\\\n",
    "where(people.name.like('%n%')).\\\n",
    "orderBy(people.age.asc()).\\\n",
    "show()\n",
    "# select: wyswietl kolumny 'name' i 'age'\n",
    "# where: uwzglednij tylko imiona (name) zaweirajace litere 'n'\n",
    "# orderBy: zbior posortuj rosnaco po kolumnie 'age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "| 30|  Andy|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gdy chcemy zobaczyć wszystkie kolumny select jest zbędny\n",
    "people.\\\n",
    "where(people.name.like('%n%')).\\\n",
    "orderBy(people.age.asc()).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyswietl imiona ludzi ze zbioru `people` starszych niz 29 lat. Wyniki posortuj alfabetycznie po kolumnie name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na zbiorach\n",
    "union - dziala jak UNION ALL w SQL. <br>\n",
    "intersect (INTERSECT z SQLa), subtract (EXCEPT z SQLa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|    age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "|     35|   Emma|\n",
      "|   null|   null|\n",
      "|     31|   null|\n",
      "|Michael|   29.0|\n",
      "|   Andy|   30.0|\n",
      "| Justin|   19.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.union(people_txt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|30.0|   Andy|\n",
      "|19.0| Justin|\n",
      "|35.0|   Emma|\n",
      "|null|   null|\n",
      "|31.0|   null|\n",
      "|29.0|Michael|\n",
      "|30.0|   Andy|\n",
      "|19.0| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.union(people_txt.select(people.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################\n",
    "\n",
    "# Zadania 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people = spark.read.json(data_path+'people.json')\n",
    "employees = spark.read.json(data_path+'employees.json')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson1 = Row(name=u'Greg', age=32)\n",
    "newPerson = Row(\"age\", \"name\")\n",
    "newPerson2 = newPerson(24, 'Alice')\n",
    "newPerson3 = newPerson(None, None)\n",
    "newPerson4 = newPerson(33, None)\n",
    "newPerson5 = newPerson(None, 'Peter')\n",
    "newPerson6 = newPerson(32, 'Peter')\n",
    "newPerson7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeopleDF = spark.createDataFrame([newPerson1, newPerson2, newPerson3, newPerson4, \n",
    "                                     newPerson5, newPerson6, newPerson7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPeople = spark.read.parquet(data_path+'allPeople.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wyswietl imiona ludzi ze zbioru `people`, o ktorych wielku nie mamy informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   null|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(people.age.isNull()).select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Na koniec zbioru `people` doklej zbiór `newPeopleDF` oraz `people_txt`. Tak utworzony DataFrame nazwij `allPeople`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wybierz ze zbioru `allPeople` te wiersze, ktore wystepuja rowniez w zbiorze `people`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Wybierz ze zbioru `allPeople` te imiona, ktore NIE wystepuja w zbiorze `people_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Usun ze zbioru `allPeople` wiersze, ktore zawieraja same braki danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Do zbioru `allPeople` dodaj kolumne 'age' zawierajaca wiek powiekszony o 1. Zmien nazwe oryginalnej kolumny 'age' na 'starting_age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Ile unikatowych rekordow znajduje sie w zbiorze `allPeople`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **⋆** Do zbioru `employees` dodaj kolumny: z pensjami po 0-10% podwyżce (dla każdego losowo z rozkładu jednostajnego) oraz z różnicą pomiędzy pensją przed i po podwyżce. Zbiór posortuj alfabetycznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praca z DataFrame'ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'\n",
    "people = spark.read.json(data_path+'people.json')\n",
    "employees = spark.read.json(data_path+'employees.json')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')\n",
    "newPeople = spark.read.parquet(data_path+'newPeople.parquet')\n",
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.select(f.min(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.select(f.min(\"age\").alias(\"min_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.withColumn(\"random\", f.randn(42)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.withColumn(\"random\", f.exp(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "inner (domyslny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.join(other=employees, on='name', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Uwaga ogólna </b><br>\n",
    "Join to stosunkowo popularna, ale kosztowna operacja.<br>\n",
    "W sytuacji, kiedy jeden z łaczonych DataFramow jest znacznie mniejszy (w szczegolnosci na tyle mały, że w całości mieści się w pamięci), zaleca sie zastosowanie <i>broadcast hash join</i>.<br>\n",
    "(Mała tabela zostanie zebrana do pamięci i wysłana do każdego noda).<br>\n",
    "W niektórych przypadkach optymalizator sam za nas zdecyduje o zastosowaniu <i>broadcast hash join</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "newPeople.join(broadcast(spark.createDataFrame([Row(age=20, name='Greg')])), on='name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Spośród osób (`people`, `newPeople`, `people_txt`), dla których mamy informacje o zarobkach (zbiór `employees`). Ile zarabia najmłodsza osoba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla każdego pracownika (`employees`), dla którego mamy informacje o wieku (`people`, `newPeople`, `people_txt`). Dodaj do pensji 0.1% za każdy rok życia. Oblicz koszt takiego 'bonusu urodzinowego' dla pracodawcy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeople.groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Przez GroupedData mamy dostep do takich funkcji jak:<br>\n",
    " avg, max, min, sum, count, agg <br>\n",
    " (dla wygody, do funkcji 'agg' mamy tez dostep bezposrednio na DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeople.groupBy().max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeople.groupBy('name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeople.groupBy('name').agg(f.min('age').alias('min_age'), f.max('age').alias('max_age'),\\\n",
    "                              f.count('name').alias('n_people')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeople.agg(f.min('age'), f.max('age'), f.count('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile jest unikatowych (występujących tylko 1 raz) imion w połączonych zbiorach `people`, `newPeople` oraz `people_txt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile lat mają osoby, których imiona występują tylko raz w połączonych zbiorach `people`, `newPeople` oraz `people_txt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Nowy DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "years = 10\n",
    "names = ['Alice', 'Betty', 'Chris', 'Dan', 'Greg']\n",
    "unique_names_count = len(names)\n",
    "names = sorted(names*years)\n",
    "year = [y for y in range(2000, 2000+years)]*len(names)\n",
    "starting_salary = [round(random.gauss(4000, 1000),2) for i in range(unique_names_count)]\n",
    "salary = [0 for i in range(years*unique_names_count)]\n",
    "salary[::years] = starting_salary\n",
    "for n in range(unique_names_count):\n",
    "    for y in range(years-1):\n",
    "        index = (years*n+1)+y\n",
    "        salary[index] = round(salary[index-1]*(1+random.gauss(0.1,0.09)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory = spark.createDataFrame([Row(name=n, year=y, salary=s) for n,y,s in zip(names, year, salary)])\n",
    "salaryHistory = salaryHistory.filter((salaryHistory['name'] != 'Greg') | (salaryHistory['year'] != 2006))\n",
    "salaryHistory = salaryHistory.union(spark.createDataFrame([Row('Alice', 3000, 2000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile razy powtarza się każde z imion w `salaryHistory`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie `salaryHistory` stworz tabelę zależności średniej, minimalnej i maksymalnej pensji od roku. Posortuj lata malejąco. Pensje podaj z dokładnością do pełnych wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "**over**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Służy do obliczania agregowanych wartości w grupach definiowanych oknem (window).<br>\n",
    "Zwraca wiele rekordow (tyle ile na wejsciu w grupie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy - definiuje podział danych na okna<br>\n",
    "orderBy - definiuje sortowanie wewnątrz każdego z okien<br>\n",
    "Frame (rangeBetween/rowsBetween) - definiuje offset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPeople = spark.read.parquet(data_path+'allPeople.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definicja 'okna'\n",
    "myWindowSpec = Window.partitionBy('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wywołanie funkcji na kazdym 'oknie'\n",
    "allPeople.withColumn('nameCount', \\\n",
    "                     f.count(allPeople['name']).over(myWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Do zbioru `salaryHistory` dodaj kolumnę 'avgSalaryDiff', która będzie zawierała różnicę pomiedzy pensją z danego roku, a średnią pensją osoby na przestrzeni wszytskich lat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy + orderBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# przykład: rank\n",
    "# - musimy zdefiniować dodatkowo sortowanie wewnątrz każdej z grup\n",
    "# - zwraca lp dla kolejnych rekordów posortowanych według zadanych kolumn\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\n",
    "ranked = f.rank().over(windowSpec)\n",
    "salaryHistory.withColumn('ranked', ranked).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory_tmp = salaryHistory.filter(salaryHistory.name.isin('Alice', 'Greg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rank, dense_rank, percent_rank, row_number\n",
    "windowSpec = Window.partitionBy('name').orderBy('year')\n",
    "ranked = (f.rank()).over(windowSpec)\n",
    "dense_rank = (f.dense_rank()).over(windowSpec)\n",
    "percent_rank = (f.percent_rank()).over(windowSpec)\n",
    "row_number = (f.row_number()).over(windowSpec)\n",
    "salaryHistory_tmp.withColumn('ranked', ranked).withColumn('dense_rank', dense_rank).\\\n",
    "withColumn('percent_rank', percent_rank).withColumn('row_number', row_number).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Inne warte uwagi funkcje: <br>\n",
    "ntile, cume_dist, first, lag, lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla zbioru `salaryHistory`, porównaj pensje ludzi pomiedzy najwcześniejszym i najpóźniejszym rokiem ich pracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy + orderBy + rangeBetween/rowsBetween**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# przykład: średnia ruchoma\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\\\n",
    ".rowsBetween(-1,1)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# przykład: średnia ze wszystkich rekordów do aktualnego włącznie\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).\\\n",
    "rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.withColumn('movingAvg', movingAvg).filter(\"name == 'Alice'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# podobny efekt uzyskamy poniższym zapytaniemm. \n",
    "# Różnica: rekordy w jednej grupie (imię, rok) nie zostaną rozdzielone \n",
    "import sys\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).\\\n",
    "rangeBetween(-sys.maxsize,0)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.withColumn('movingAvg', movingAvg).filter(\"name == 'Alice'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################\n",
    "\n",
    "# Zadania 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Ile lat mają osoby, których imiona występują tylko raz w połączonych zbiorach `people`, `newPeople` oraz `people_txt`? Rozwiązując problem zastosuj window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Czy komukolwiek obniżyła się pensja w stosunku do roku poprzedniego? <br>\n",
    "a. Ile osób było kiedykolwiek w takiej sytuacji?<br>\n",
    "b. Jaki jest rozkład częstości takich przypadków w zależności od roku?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3. Oblicz różnicę w pensjach w stosuku do <br>\n",
    "a. najwyższej pensji danej osoby<br> \n",
    "b. drugiej najwyższej pensji danej osoby<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark wspiera ANSI SQL 2003 (SQL3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby użyć zbioru danych w zapytaniu SQL musimy go najpierw zarejestrować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.createOrReplaceTempView('salaryHistory')\n",
    "# salaryHistory.registerTempTable('salaryHistorySQL') - wersja dla sparka 1.x\n",
    "# view NIE jest 'persisted in memory', to nadal nie jest akcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from salaryHistory limit 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(spark.sql('select * from salaryHistory limit 2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select name, avg(salary) avg_sal from salaryHistory group by name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select name, salary, avg(salary) over (partition by name) avg_sal from salaryHistory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"salaryHistory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs\n",
    "Poza funkcjami dostępnymi w module pyspark.sql.functions, można też tworzyć własne (User Defined Functions) i używać ich w zapytaniach SparkSQL <br>\n",
    "<i>Uwaga:</i> Po UDFy sięgamy w ostateczności - optymalizacja <br>\n",
    "<i>Uwaga:</i> UDF napisany w Pythonie (lub R) bedzie mial gorszy performance niż UDF napisany w Scali lub Javie. Ale możemy nasz UDF napisać w Scali/Javie i zarejestrować do użycia w Pythonie. <br>\n",
    "Blogpost o tym jak w pySparku używać UDF napisanej w scali: <br>\n",
    "https://medium.com/@ingwbaa/using-scala-udfs-in-pyspark-b70033dd69b9 <br>\n",
    "Blogpost o ulepszeniach w UDF (pandas UDF) dla pySparka od Sparku 2.3: <br>\n",
    "https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def udfPower3(value):\n",
    "    return(value**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udfPower3(3.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aby stosować zdefiniowaną funkcję, musimy ja zarejestrować. (Driver roześle funkcje do wszystkich egzekutorów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power3 = f.udf(udfPower3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.select(power3(salaryHistory.salary)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark wspiera Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL wspiera HiveQL. <br>\n",
    "Spark SQL wspiera rownież wczytawanie/zapisywanie danych z/do Apache Hive.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiecej informacji na temat integracji z Hive:<br>\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#compatibility-with-apache-hive <br>\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
