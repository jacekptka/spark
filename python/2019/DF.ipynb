{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie DataFrame'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "people = spark.read.json(data_path+'people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolekcja Row'ów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1 = Row(name='Greg', age=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'age' in newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson = Row(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson2 = newPerson(24, 'Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson3 = newPerson(None, None)\n",
    "newPerson4 = newPerson(33, None)\n",
    "newPerson5 = newPerson(None, 'Peter')\n",
    "newPerson6 = newPerson(32, 'Peter')\n",
    "newPerson7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF = spark.createDataFrame([newPerson1, newPerson2, newPerson3, newPerson4, \n",
    "                                     newPerson5, newPerson6, newPerson7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne lokalne kolekcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typy danych: http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "Kilka podstawowych: IntegerType, DoubleType, FloatType, StringType, BooleanType, NullType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicja schematu\n",
    "# StructType ~ Row\n",
    "schema = StructType([StructField(\"V1\", IntegerType()), StructField(\"V2\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lokalna kolekcja - lista list\n",
    "df = spark.createDataFrame([[1,2],[3,4]], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przechodzenie RDD <-> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF -> RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(people.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.rdd.map(tuple).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD -> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.rdd.map(tuple).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do toDF można podać schemat\n",
    "schema = StructType([StructField(\"A\", IntegerType()), StructField(\"B\", StringType())])\n",
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na dwa sposoby stwórz DataFrame z 3 wierszami i 3 kolumnami - dwie typu string, jedna numeryczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Z podanego RDD utwórz DataFrame z nazwanymi kolumnami `name` i `age` oraz odpowiednimi typami (string i int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile(data_path+\"people.txt\")\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praca z DataFrame'ami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogolne wiadomosci na temat danych\n",
    "printSchema, show, columns, dtypes <br>\n",
    "Znane z RDD np: count, take, head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba wierszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kolumn wraz z typami danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odwolania do poszczegolnych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dodanie/usuniecie kolumny\n",
    "withColumn, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.withColumn(colName = 'ageNextYear', col = people.age +1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiany nazwy kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podstawowe statystyki kolumn w DataFrame'ie.\n",
    "describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.describe('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braki danych\n",
    "isNull, isNotNull<br>\n",
    "fillna, dropna, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.filter(newPeopleDF.age.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.filter(newPeopleDF.age.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.fillna(-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.fillna({'age':-1, 'name':'unknown'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.fillna({'name':'unknown'}).replace('unknown', 'NN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF.dropna(subset='age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje wprost ze skladni SQL \n",
    "select, where (alias filter), orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.\\\n",
    "select('name', 'age').\\\n",
    "where(people.name.like('%n%')).\\\n",
    "orderBy(people.age.asc()).\\\n",
    "show()\n",
    "# select: wyswietl kolumny 'name' i 'age'\n",
    "# where: uwzglednij tylko imiona (name) zaweirajace litere 'n'\n",
    "# orderBy: zbior posortuj rosnaco po kolumnie 'age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdy chcemy zobaczyć wszystkie kolumny select jest zbędny\n",
    "people.\\\n",
    "where(people.name.like('%n%')).\\\n",
    "orderBy(people.age.asc()).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyswietl imiona ludzi ze zbioru `people` starszych niz 29 lat. Wyniki posortuj alfabetycznie po kolumnie name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na zbiorach\n",
    "union - dziala jak UNION ALL w SQL. <br>\n",
    "intersect (INTERSECT z SQLa), subtract (EXCEPT z SQLa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.union(people_txt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.union(people_txt.select(people.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################\n",
    "\n",
    "# Zadania 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.json(data_path+'people.json')\n",
    "employees = spark.read.json(data_path+'employees.json')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson1 = Row(name=u'Greg', age=32)\n",
    "newPerson = Row(\"age\", \"name\")\n",
    "newPerson2 = newPerson(24, 'Alice')\n",
    "newPerson3 = newPerson(None, None)\n",
    "newPerson4 = newPerson(33, None)\n",
    "newPerson5 = newPerson(None, 'Peter')\n",
    "newPerson6 = newPerson(32, 'Peter')\n",
    "newPerson7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeopleDF = spark.createDataFrame([newPerson1, newPerson2, newPerson3, newPerson4, \n",
    "                                     newPerson5, newPerson6, newPerson7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPeople = spark.read.parquet(data_path+'allPeople.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wyswietl imiona ludzi ze zbioru `people`, o ktorych wielku nie mamy informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Na koniec zbioru `people` doklej zbiór `newPeopleDF` oraz `people_txt`. Tak utworzony DataFrame nazwij `allPeople`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wybierz ze zbioru `allPeople` te wiersze, ktore wystepuja rowniez w zbiorze `people`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Wybierz ze zbioru `allPeople` te imiona, ktore NIE wystepuja w zbiorze `people_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Usun ze zbioru `allPeople` wiersze, ktore zawieraja same braki danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Do zbioru `allPeople` dodaj kolumne 'age' zawierajaca wiek powiekszony o 1. Zmien nazwe oryginalnej kolumny 'age' na 'starting_age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Ile unikatowych rekordow znajduje sie w zbiorze `allPeople`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **⋆** Do zbioru `employees` dodaj kolumny: z pensjami po 0-10% podwyżce (dla każdego losowo z rozkładu jednostajnego) oraz z różnicą pomiędzy pensją przed i po podwyżce. Zbiór posortuj alfabetycznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praca z DataFrame'ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'\n",
    "people = spark.read.json(data_path+'people.json')\n",
    "employees = spark.read.json(data_path+'employees.json')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')\n",
    "newPeople = spark.read.parquet(data_path+'newPeople.parquet')\n",
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(f.min(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(f.min(\"age\").alias(\"min_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.withColumn(\"random\", f.randn(42)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.withColumn(\"random\", f.exp(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "inner (domyslny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(other=employees, on='name', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Uwaga ogólna </b><br>\n",
    "Join to stosunkowo popularna, ale kosztowna operacja.<br>\n",
    "W sytuacji, kiedy jeden z łaczonych DataFramow jest znacznie mniejszy (w szczegolnosci na tyle mały, że w całości mieści się w pamięci), zaleca sie zastosowanie <i>broadcast hash join</i>.<br>\n",
    "(Mała tabela zostanie zebrana do pamięci i wysłana do każdego noda).<br>\n",
    "W niektórych przypadkach optymalizator sam za nas zdecyduje o zastosowaniu <i>broadcast hash join</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "newPeople.join(broadcast(spark.createDataFrame([Row(age=20, name='Greg')])), on='name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Spośród osób (`people`, `newPeople`, `people_txt`), dla których mamy informacje o zarobkach (zbiór `employees`). Ile zarabia najmłodsza osoba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla każdego pracownika (`employees`), dla którego mamy informacje o wieku (`people`, `newPeople`, `people_txt`). Dodaj do pensji 0.1% za każdy rok życia. Oblicz koszt takiego 'bonusu urodzinowego' dla pracodawcy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeople.groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Przez GroupedData mamy dostep do takich funkcji jak:<br>\n",
    " avg, max, min, sum, count, agg <br>\n",
    " (dla wygody, do funkcji 'agg' mamy tez dostep bezposrednio na DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeople.groupBy().max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeople.groupBy('name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeople.groupBy('name').agg(f.min('age').alias('min_age'), f.max('age').alias('max_age'),\\\n",
    "                              f.count('name').alias('n_people')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPeople.agg(f.min('age'), f.max('age'), f.count('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile jest unikatowych (występujących tylko 1 raz) imion w połączonych zbiorach `people`, `newPeople` oraz `people_txt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile lat mają osoby, których imiona występują tylko raz w połączonych zbiorach `people`, `newPeople` oraz `people_txt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Nowy DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "years = 10\n",
    "names = ['Alice', 'Betty', 'Chris', 'Dan', 'Greg']\n",
    "unique_names_count = len(names)\n",
    "names = sorted(names*years)\n",
    "year = [y for y in range(2000, 2000+years)]*len(names)\n",
    "starting_salary = [round(random.gauss(4000, 1000),2) for i in range(unique_names_count)]\n",
    "salary = [0 for i in range(years*unique_names_count)]\n",
    "salary[::years] = starting_salary\n",
    "for n in range(unique_names_count):\n",
    "    for y in range(years-1):\n",
    "        index = (years*n+1)+y\n",
    "        salary[index] = round(salary[index-1]*(1+random.gauss(0.1,0.09)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory = spark.createDataFrame([Row(name=n, year=y, salary=s) for n,y,s in zip(names, year, salary)])\n",
    "salaryHistory = salaryHistory.filter((salaryHistory['name'] != 'Greg') | (salaryHistory['year'] != 2006))\n",
    "salaryHistory = salaryHistory.union(spark.createDataFrame([Row('Alice', 3000, 2000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile razy powtarza się każde z imion w `salaryHistory`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie `salaryHistory` stworz tabelę zależności średniej, minimalnej i maksymalnej pensji od roku. Posortuj lata malejąco. Pensje podaj z dokładnością do pełnych wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "**over**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Służy do obliczania agregowanych wartości w grupach definiowanych oknem (window).<br>\n",
    "Zwraca wiele rekordow (tyle ile na wejsciu w grupie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy - definiuje podział danych na okna<br>\n",
    "orderBy - definiuje sortowanie wewnątrz każdego z okien<br>\n",
    "Frame (rangeBetween/rowsBetween) - definiuje offset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPeople = spark.read.parquet(data_path+'allPeople.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicja 'okna'\n",
    "myWindowSpec = Window.partitionBy('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wywołanie funkcji na kazdym 'oknie'\n",
    "allPeople.withColumn('nameCount', \\\n",
    "                     f.count(allPeople['name']).over(myWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Do zbioru `salaryHistory` dodaj kolumnę 'avgSalaryDiff', która będzie zawierała różnicę pomiedzy pensją z danego roku, a średnią pensją osoby na przestrzeni wszytskich lat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy + orderBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład: rank\n",
    "# - musimy zdefiniować dodatkowo sortowanie wewnątrz każdej z grup\n",
    "# - zwraca lp dla kolejnych rekordów posortowanych według zadanych kolumn\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\n",
    "ranked = f.rank().over(windowSpec)\n",
    "salaryHistory.withColumn('ranked', ranked).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory_tmp = salaryHistory.filter(salaryHistory.name.isin('Alice', 'Greg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, dense_rank, percent_rank, row_number\n",
    "windowSpec = Window.partitionBy('name').orderBy('year')\n",
    "ranked = (f.rank()).over(windowSpec)\n",
    "dense_rank = (f.dense_rank()).over(windowSpec)\n",
    "percent_rank = (f.percent_rank()).over(windowSpec)\n",
    "row_number = (f.row_number()).over(windowSpec)\n",
    "salaryHistory_tmp.withColumn('ranked', ranked).withColumn('dense_rank', dense_rank).\\\n",
    "withColumn('percent_rank', percent_rank).withColumn('row_number', row_number).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Inne warte uwagi funkcje: <br>\n",
    "ntile, cume_dist, first, lag, lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla zbioru `salaryHistory`, porównaj pensje ludzi pomiedzy najwcześniejszym i najpóźniejszym rokiem ich pracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partitionBy + orderBy + rangeBetween/rowsBetween**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład: średnia ruchoma\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\\\n",
    ".rowsBetween(-1,1)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład: średnia ze wszystkich rekordów do aktualnego włącznie\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).\\\n",
    "rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory.withColumn('movingAvg', movingAvg).filter(\"name == 'Alice'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podobny efekt uzyskamy poniższym zapytaniemm. \n",
    "# Różnica: rekordy w jednej grupie (imię, rok) nie zostaną rozdzielone \n",
    "import sys\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).\\\n",
    "rangeBetween(-sys.maxsize,0)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory.withColumn('movingAvg', movingAvg).filter(\"name == 'Alice'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################\n",
    "\n",
    "# Zadania 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Ile lat mają osoby, których imiona występują tylko raz w połączonych zbiorach `people`, `newPeople` oraz `people_txt`? Rozwiązując problem zastosuj window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Czy komukolwiek obniżyła się pensja w stosunku do roku poprzedniego? <br>\n",
    "a. Ile osób było kiedykolwiek w takiej sytuacji?<br>\n",
    "b. Jaki jest rozkład częstości takich przypadków w zależności od roku?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3. Oblicz różnicę w pensjach w stosuku do <br>\n",
    "a. najwyższej pensji danej osoby<br> \n",
    "b. drugiej najwyższej pensji danej osoby<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark wspiera ANSI SQL 2003 (SQL3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby użyć zbioru danych w zapytaniu SQL musimy go najpierw zarejestrować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory.createOrReplaceTempView('salaryHistory')\n",
    "# salaryHistory.registerTempTable('salaryHistorySQL') - wersja dla sparka 1.x\n",
    "# view NIE jest 'persisted in memory', to nadal nie jest akcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from salaryHistory limit 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark.sql('select * from salaryHistory limit 2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select name, avg(salary) avg_sal from salaryHistory group by name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select name, salary, avg(salary) over (partition by name) avg_sal from salaryHistory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"salaryHistory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs\n",
    "Poza funkcjami dostępnymi w module pyspark.sql.functions, można też tworzyć własne (User Defined Functions) i używać ich w zapytaniach SparkSQL <br>\n",
    "<i>Uwaga:</i> Po UDFy sięgamy w ostateczności - optymalizacja <br>\n",
    "<i>Uwaga:</i> UDF napisany w Pythonie (lub R) bedzie mial gorszy performance niż UDF napisany w Scali lub Javie. Ale możemy nasz UDF napisać w Scali/Javie i zarejestrować do użycia w Pythonie. <br>\n",
    "Blogpost o tym jak w pySparku używać UDF napisanej w scali: <br>\n",
    "https://medium.com/@ingwbaa/using-scala-udfs-in-pyspark-b70033dd69b9 <br>\n",
    "Blogpost o ulepszeniach w UDF (pandas UDF) dla pySparka od Sparku 2.3: <br>\n",
    "https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udfPower3(value):\n",
    "    return(value**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfPower3(3.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aby stosować zdefiniowaną funkcję, musimy ja zarejestrować. (Driver roześle funkcje do wszystkich egzekutorów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power3 = f.udf(udfPower3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory.select(power3(salaryHistory.salary)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark wspiera Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL wspiera HiveQL. <br>\n",
    "Spark SQL wspiera rownież wczytawanie/zapisywanie danych z/do Apache Hive.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiecej informacji na temat integracji z Hive:<br>\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#compatibility-with-apache-hive <br>\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
